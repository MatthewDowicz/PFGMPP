{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752a774d-47fe-4bd1-8bf6-586e6e118e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from typing import Any, Sequence, Optional, Tuple, Iterator, Dict, Callable, Union, Type, List\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "# Changing fonts to be latex typesetting\n",
    "from matplotlib import rcParams\n",
    "rcParams['mathtext.fontset'] = 'dejavuserif'\n",
    "rcParams['font.family'] = 'serif'\n",
    "\n",
    "# JAX/Flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax import linen as nn\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, orbax_utils\n",
    "import optax\n",
    "import orbax\n",
    "import wandb\n",
    "wandb_dir = os.path.join(os.path.expanduser('~'), \"PFGMPP\")\n",
    "os.environ[\"WANDB_DIR\"] = os.path.abspath(wandb_dir)\n",
    "from tqdm import tqdm\n",
    "\n",
    "from visualization import visualize as vis\n",
    "from data import data_functions as df\n",
    "from models import model_architecture as march\n",
    "from models import train_model as trm\n",
    "from models import generate_model as gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f6f869-85df-47f2-9cf7-7b123884d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    initial_feature: int = 16\n",
    "    out_feature: int = 2\n",
    "    std_data: float = 0.5\n",
    "    # embedding_dim: int = 32\n",
    "\n",
    "    def setup(self):\n",
    "        # Initial dense layer\n",
    "        self.inc = nn.Dense(self.initial_feature) # 2->64\n",
    "        # Activation\n",
    "        self.act = nn.relu\n",
    "        # Final Output Layer\n",
    "        self.outc = nn.Dense(self.out_feature)\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        # Preconditioning terms\n",
    "        t = t.squeeze()\n",
    "        c_out = t * self.std_data / jnp.sqrt(self.std_data**2 + t**2)\n",
    "        c_skip = self.std_data**2 / (self.std_data**2 + t**2)\n",
    "\n",
    "        # Sampling the noise and embedding the noise via positional encoding\n",
    "        t = jnp.log(t.flatten()) / 4.\n",
    "        t = self.pos_encoding(t, self.initial_feature)\n",
    "        x_orig = x\n",
    "\n",
    "        # First layer + noise embedding\n",
    "        x_1 = self.inc(x) + t\n",
    "        x_2 = self.act(x)\n",
    "        # Second layer\n",
    "        x_3 = self.inc(x_2)\n",
    "        x_4 = self.act(x)\n",
    "        # Third layer\n",
    "        x_5 = self.inc(x_4)\n",
    "        x_6 = self.act(x)\n",
    "        # Output layer\n",
    "        output = self.outc(x_6)\n",
    "\n",
    "        # Reshape c_out & c_skip to match dimensions for broadcasting\n",
    "        c_out = jnp.reshape(c_out, (-1,1))\n",
    "        c_skip = jnp.reshape(c_skip, (-1,1))\n",
    "        return c_out * output + c_skip * x_orig\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        t = jnp.expand_dims(t, axis=-1)  # Add an additional dimension to t\n",
    "        inv_freq = 1.0 / (10000 ** (jnp.arange(0, channels, 2).astype(jnp.float32) / channels))\n",
    "        pos_enc_a = jnp.sin(t * inv_freq)\n",
    "        pos_enc_b = jnp.cos(t * inv_freq)\n",
    "        pos_enc = jnp.concatenate([pos_enc_a, pos_enc_b], axis=-1)\n",
    "        return pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753e0574-e5b8-45a7-9fce-9b564921ed40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'minimize', 'name': 'Train Loss'},\n",
      " 'parameters': {'D': {'values': [10, 100, 1000, 10000]},\n",
      "                'N': {'value': 2},\n",
      "                'batch_size': {'value': 64},\n",
      "                'epochs': {'value': 60},\n",
      "                'gen_data_key': {'value': 21},\n",
      "                'initial_feature': {'values': [16, 32, 64, 128, 256]},\n",
      "                'learning_rate': {'values': [0.001, 0.0001]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'out_feature': {'value': 2},\n",
      "                'size': {'value': 2000},\n",
      "                'std_data': {'values': [0.5, 1]},\n",
      "                'train_model_key': {'value': 47}}}\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'Train Loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer': {\n",
    "            'value': 'adam'\n",
    "        },\n",
    "        # 'embedding_dim': {\n",
    "        #     'values': [16, 32, 64, 128, 256]\n",
    "        # },\n",
    "        'epochs': {\n",
    "            'value': 60\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'value': 64\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [1e-3, 1e-4]\n",
    "        },\n",
    "        'N': {\n",
    "            'value': 2\n",
    "        },\n",
    "        'D': {\n",
    "            'values': [10, 100, 1000, 10000]\n",
    "        },\n",
    "        'std_data': {\n",
    "            'values': [0.5, 1]\n",
    "        },\n",
    "        'initial_feature': {\n",
    "            'values': [16, 32, 64, 128, 256]\n",
    "        },\n",
    "        'out_feature': {\n",
    "            'value': 2\n",
    "        },\n",
    "        'size': {\n",
    "            'value': 2000\n",
    "        },\n",
    "        'gen_data_key': {\n",
    "            'value': 21\n",
    "        },\n",
    "        'train_model_key': {\n",
    "            'value': 47\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8214b961-a0a0-48c9-a3c1-7e7fe40c5193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_sweep(train_loader,\n",
    "                      model,\n",
    "                      state,\n",
    "                      config,\n",
    "                      dir_name,\n",
    "                      project_name,\n",
    "                      key_seed=47,\n",
    "                      wandb_logging=True):\n",
    "    \"\"\"\n",
    "    Train a machine learning model with optional Weights & Biases (wandb) logging.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_loader: \n",
    "        A data loader providing the training data.\n",
    "    model: \n",
    "        The model to be trained.\n",
    "    state: \n",
    "        The initial state of the model.\n",
    "    config: dict\n",
    "        A dictionary containing configuration parameters for training, such as learning rate, batch size etc.\n",
    "    wandb_logging: bool\n",
    "        If True, training progress is logged using wandb.\n",
    "        Default is False.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        model: The trained model.\n",
    "        state: The final state of the model after training.\n",
    "    \"\"\"\n",
    "    # Initialize a var to hold the best test loss seen so far\n",
    "    best_train_loss = float('inf')\n",
    "    \n",
    "    # Start the training loop\n",
    "    for epoch in tqdm(range(config['epochs'])):\n",
    "        # Initialize a list to store all batch-level metrics\n",
    "        batch_metrics = []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            var_data = 0.2 ** 2 + 1 ** 2\n",
    "            sigma_data = jnp.sqrt(var_data)\n",
    "            \n",
    "            # Prepare the data\n",
    "            batch = jax.device_put(batch)\n",
    "            \n",
    "            # Normalize the data such that it's std is 1\n",
    "            batch /= sigma_data\n",
    "            \n",
    "            # Update the model\n",
    "            train_step_jit = jax.jit(trm.train_step, static_argnums=(3,4,5))\n",
    "            state, batch_loss = train_step_jit(state, batch, config['std_data'], config['D'], config['N'], config['train_model_key'])    \n",
    "\n",
    "            # Store the batch-level metric in the list\n",
    "            batch_metrics.append({'Train Loss': batch_loss})\n",
    "\n",
    "        # Use accumulate_metrics to calculate average metrics for the epoch\n",
    "        epoch_metrics = trm.accumulate_metrics(batch_metrics)\n",
    "        \n",
    "\n",
    "        # If the train loss for this epoch is better than the previous best,\n",
    "        # save the model\n",
    "        if epoch_metrics['Train Loss'] < best_train_loss:\n",
    "            best_train_loss = epoch_metrics['Train Loss'] # Update the best train loss\n",
    "            checkpt_dir = dir_name # dir where models are saved to\n",
    "            trm.save_checkpoint(checkpt_dir, state, epoch, project_name)\n",
    "\n",
    "        \n",
    "        # If wandb logging is enabled, log metrics\n",
    "        if wandb_logging:\n",
    "            print('Epoch Metrics =', epoch_metrics)\n",
    "            wandb.log(epoch_metrics)\n",
    "\n",
    "    return model, state\n",
    "\n",
    "def train_sweep(config=None, project_name='pfgmpp_test', dir_name='PFGMPP/saved_models/toy'):\n",
    "    \n",
    "    # Create random PRNG keys for training\n",
    "    init_rng = random.PRNGKey(0)\n",
    "    rng, subkey1, subkey2, subkey3 = random.split(init_rng, num=4)\n",
    "\n",
    "    # Get the absolute path of the saved_models/toy directory\n",
    "    dir_name = os.path.join(os.path.expanduser('~'), str(dir_name)) #FIX THIS HARDCODE\n",
    "    \n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, dir=dir_name):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Generate dataset of 2D Gaussians\n",
    "        X = df.generate_data(config['gen_data_key'], config['size'])\n",
    "        \n",
    "        # Make the data suitable for a JAX Dataloader\n",
    "        train_dataset = df.JaxDataset(X=X)\n",
    "        train_loader = df.NumpyLoader(dataset=train_dataset,\n",
    "                                      batch_size=config['batch_size'],\n",
    "                                      shuffle=True)\n",
    "        batch = next(iter(train_loader))\n",
    "        \n",
    "        # Get the rng & model instantiated\n",
    "        model = MLP(initial_feature=config['initial_feature'],\n",
    "                    std_data=config['std_data'],\n",
    "                    out_feature=config['out_feature'])#,\n",
    "                    #embedding_dim=config['embedding_dim'])\n",
    "        \n",
    "        # Sample the noise distribution\n",
    "        rnd_normal = random.normal(subkey2, shape=(batch.shape[0], 1))\n",
    "        t = jnp.exp(rnd_normal * 1.2 - 1.2)  \n",
    "    \n",
    "    \n",
    "        # Initialize the models state\n",
    "        state = trm.init_train_state(model=model,\n",
    "                                     random_key=subkey3,\n",
    "                                     x_shape=batch.shape,\n",
    "                                     t_shape=t.shape,\n",
    "                                     learning_rate=config['learning_rate'])\n",
    "        \n",
    "        model, state = train_model_sweep(train_loader,\n",
    "                                         model,\n",
    "                                         state,\n",
    "                                         config,\n",
    "                                         key_seed=47,\n",
    "                                         wandb_logging=True,\n",
    "                                         project_name=project_name,\n",
    "                                         dir_name=dir_name)\n",
    "\n",
    "        del state\n",
    "        del model\n",
    "        del train_dataset\n",
    "        del train_loader\n",
    "        del X\n",
    "        del t \n",
    "        del batch\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3a005b8-b837-4c9f-8fee-9ecc8326c17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 4s01fuhj\n",
      "Sweep URL: https://wandb.ai/mdowicz/simple_PFGM%2B%2B/sweeps/4s01fuhj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c4eo9307 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tD: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tN: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgen_data_key: 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_feature: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_feature: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize: 2000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstd_data: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_model_key: 47\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmdowicz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mdowicz/PFGMPP/saved_models/toy/wandb/run-20230816_221718-c4eo9307</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mdowicz/simple_PFGM%2B%2B/runs/c4eo9307' target=\"_blank\">tough-sweep-1</a></strong> to <a href='https://wandb.ai/mdowicz/simple_PFGM%2B%2B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mdowicz/simple_PFGM%2B%2B/sweeps/4s01fuhj' target=\"_blank\">https://wandb.ai/mdowicz/simple_PFGM%2B%2B/sweeps/4s01fuhj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mdowicz/simple_PFGM%2B%2B' target=\"_blank\">https://wandb.ai/mdowicz/simple_PFGM%2B%2B</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mdowicz/simple_PFGM%2B%2B/sweeps/4s01fuhj' target=\"_blank\">https://wandb.ai/mdowicz/simple_PFGM%2B%2B/sweeps/4s01fuhj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mdowicz/simple_PFGM%2B%2B/runs/c4eo9307' target=\"_blank\">https://wandb.ai/mdowicz/simple_PFGM%2B%2B/runs/c4eo9307</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "  2%|▏         | 1/60 [00:05<05:53,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 152.40222}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/60 [00:10<04:43,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 147.66794}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/60 [00:14<04:16,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 148.5904}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/60 [00:18<04:02,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 150.38896}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/60 [00:22<03:52,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 147.97176}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6/60 [00:26<03:44,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 147.68816}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7/60 [00:30<03:37,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 147.97417}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 8/60 [00:34<03:32,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 152.9771}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9/60 [00:38<03:27,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 150.0151}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 10/60 [00:42<03:23,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 147.89035}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11/60 [00:46<03:18,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 147.88747}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 12/60 [00:50<03:14,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 143.82253}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 13/60 [00:54<03:10,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 145.91238}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 14/60 [00:58<03:06,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Metrics = {'Train Loss': 147.59827}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
      " 23%|██▎       | 14/60 [00:59<03:15,  4.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"simple_PFGM++\")\n",
    "wandb.agent(sweep_id, train_sweep, count=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b77c9ee-7690-4be1-8f3d-bd3947109e11",
   "metadata": {},
   "source": [
    "# Old Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e8717d9-2699-4711-8660-cf54cad1e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     depth: int = 4\n",
    "#     initial_feature: int = 64\n",
    "#     output_feature: int = 2\n",
    "#     std_data: float = 0.5\n",
    "#     group_norm: bool = False\n",
    "\n",
    "#     def setup(self):\n",
    "#         # Encoder\n",
    "#         self.down_blocks = [DoubleConvolution(self.initial_filters * 2**i) for i in range(self.depth)]\n",
    "#         self.downsamples = [DownSample() for _ in range(self.depth)]\n",
    "        \n",
    "#         # Bottleneck \n",
    "#         self.bottleneck_block = DoubleConvolution(self.initial_filters * 2**self.depth)\n",
    "\n",
    "#         # Decoder\n",
    "#         self.up_samples = [UpSample(self.initial_filters * 2**(i-1)) for i in range(self.depth, 0, -1)]\n",
    "#         self.up_blocks = [DoubleConvolution(self.initial_filters * 2**i) for i in range(self.depth-1, -1, -1)]\n",
    "\n",
    "#         # Final Convolutional Layer\n",
    "#         self.final_conv = nn.Conv(self.output_channels, \n",
    "#                                   kernel_size=(1, 1),\n",
    "#                                   strides=(1, 1),\n",
    "#                                   padding=\"SAME\")\n",
    "\n",
    "#     def __call__(self, x):\n",
    "#         skip_connections = []\n",
    "        \n",
    "#         # Encoder path\n",
    "#         for i in range(self.depth):\n",
    "#             x = self.down_blocks[i](x)\n",
    "#             skip_connections.append(x)\n",
    "#             x = self.downsamples[i](x)\n",
    "#             # print(f'Encoder{i+1} x.shape =', x.shape)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         x = self.bottleneck_block(x)\n",
    "#         # print('Bottleneck x.shape =', x.shape)\n",
    "\n",
    "#         # Decoder path\n",
    "#         for i in range(self.depth):\n",
    "#             x = self.up_samples[i](x)\n",
    "#             x = jnp.concatenate([x, skip_connections.pop()], axis=-1)\n",
    "#             # print(f'Skip_connection{i+1} x.shape =', x.shape)\n",
    "#             x = self.up_blocks[i](x)\n",
    "#             # print(f'Decoder{i+1} x.shape =', x.shape)\n",
    "\n",
    "#         # Final Convolution layer\n",
    "#         x = self.final_conv(x)\n",
    "#         # print('Final x.shape =', x.shape)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2203b27-0f8f-421b-836d-3bfca27141f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# class DoubleDense(nn.Module):\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     features: int\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# class Down(nn.Module):\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     features: int\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43ma\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     def setup(self):\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#         self.dense1 = DoubleDense(self.features)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#         self.emb_layer = nn.Dense(self.features)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#         pos_enc = jnp.concatenate([pos_enc_a, pos_enc_b], axis=-1)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#         return pos_enc\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "# class DoubleDense(nn.Module):\n",
    "#     features: int\n",
    "\n",
    "#     def setup(self):\n",
    "#         self.dense1 = nn.Dense(self.features)\n",
    "#         self.act1 = nn.swish\n",
    "#         self.dense2 = nn.Dense(self.features)\n",
    "#         self.act2 = nn.swish\n",
    "\n",
    "#     def __call__(self, x):\n",
    "#         x = self.dense1(x)\n",
    "#         x = self.act1(x)\n",
    "#         x = self.dense2(x)\n",
    "#         x = self.act2(x)\n",
    "#         return x\n",
    "\n",
    "# class Down(nn.Module):\n",
    "#     features: int\n",
    "a\n",
    "#     def setup(self):\n",
    "#         self.dense1 = DoubleDense(self.features)\n",
    "#         self.emb_layer = nn.Dense(self.features)\n",
    "\n",
    "#     def __call__(self, x, t):\n",
    "#         x = self.dense1(x)\n",
    "#         emb = self.emb_layer(t)\n",
    "#         return x + emb\n",
    "\n",
    "\n",
    "# class Up(nn.Module):\n",
    "#     features: int\n",
    "\n",
    "#     def setup(self):\n",
    "#         self.dense1 = DoubleDense(self.features)\n",
    "#         self.emb_layer = nn.Dense(self.features)\n",
    "\n",
    "#     def __call__(self, x, skip_x, t):\n",
    "#         x = jnp.concatenate([skip_x, x], axis=-1)\n",
    "#         x = self.dense1(x)\n",
    "#         emb = self.emb_layer(t)\n",
    "#         return x + emb\n",
    "\n",
    "\n",
    "# class UNet(nn.Module):\n",
    "#     depth: int = 4\n",
    "#     initial_feature: int = 64\n",
    "#     out_feature: int = 2\n",
    "#     std_data: float = 0.5\n",
    "#     embedding_dim: int = 64\n",
    "\n",
    "#     def setup(self):\n",
    "#         # Initial dense layer\n",
    "#         self.inc = DoubleDense(self.initial_feature) # 2->64\n",
    "\n",
    "#         # Encoder Block (Downsampling)\n",
    "#         for i in range(1, self.depth):\n",
    "#             features = self.initial_feature * (2 ** i)\n",
    "#             # print('Encoder features', features)\n",
    "#             setattr(self, f'down{i}', Down(features))\n",
    "\n",
    "#         # Bottleneck Layers\n",
    "#         bottleneck_features = self.initial_feature * (2 ** (self.depth-1))\n",
    "#         self.bot1 = DoubleDense(bottleneck_features) # 512->512\n",
    "#         self.bot2 = DoubleDense(bottleneck_features * 2) # 512->1024\n",
    "#         self.bot3 = DoubleDense(bottleneck_features) # 1024->512\n",
    "\n",
    "#         # Decoder Block (Upsampling)\n",
    "#         for i in reversed(range(0, self.depth-1)):\n",
    "#             features = self.initial_feature * (2 ** i)\n",
    "#             setattr(self, f'up{i}', Up(features))\n",
    "\n",
    "#         # Final Output Layer\n",
    "#         self.outc = nn.Dense(self.out_feature)\n",
    "\n",
    "#     def __call__(self, x, t):\n",
    "#         # Preconditioning terms\n",
    "#         t = t.squeeze()\n",
    "#         c_out = t * self.std_data / jnp.sqrt(self.std_data**2 + t**2)\n",
    "#         c_skip = self.std_data**2 / (self.std_data**2 + t**2)\n",
    "\n",
    "#         # Sampling the noise and embedding the noise via positional encoding\n",
    "#         t = jnp.log(t.flatten()) / 4.\n",
    "#         t = self.pos_encoding(t, self.embedding_dim)\n",
    "#         x_orig = x\n",
    "\n",
    "#         skip_connections = []\n",
    "\n",
    "#         # Pass through the initial layer\n",
    "#         x = self.inc(x) # 2 -> 64\n",
    "#         skip_connections.append(x) # Store the output for skip connection\n",
    "\n",
    "#         # Pass through the dynamic encoder layers\n",
    "#         for i in range(1, self.depth):\n",
    "#             x = getattr(self, f'down{i}')(x, t)\n",
    "#             skip_connections.append(x) # Store the outputs for skip connections\n",
    "            \n",
    "#         # Pass through the bottleneck layers\n",
    "#         x = self.bot1(x) # 256 -> 512\n",
    "#         x = self.bot2(x) # 512 -> 1024\n",
    "#         x = self.bot3(x) # 1024 -> 512\n",
    "\n",
    "#         # Pass through the dynamic decoder (upsampling) layers\n",
    "#         skip_connections.pop() # THIS IS FOR TESTING PURPOSES FOUND THERE WAS A DISCREPANCY\n",
    "#                        # WITH THE NUMBER OF PARAMS BETWEEN STATIC AND DYNAMIC MODEL\n",
    "#                        # DUE TO THE SKIP CONNECTIONS TAKING WRONG DIMENSIONAL DATA\n",
    "#         for i in reversed(range(0, self.depth-1)):\n",
    "#             skip_output = skip_connections.pop() # Retrieve last stored output\n",
    "#             x = getattr(self, f'up{i}')(x, skip_output, t)\n",
    "\n",
    "#         # Pass through the final output layer\n",
    "#         output = self.outc(x) # 64 -> 2\n",
    "\n",
    "#         # Reshape c_out & c_skip to match dimensions for broadcasting\n",
    "#         c_out = jnp.reshape(c_out, (-1,1))\n",
    "#         c_skip = jnp.reshape(c_skip, (-1,1))\n",
    "#         return c_out * output + c_skip * x_orig\n",
    "\n",
    "#     def pos_encoding(self, t, channels):\n",
    "#         t = jnp.expand_dims(t, axis=-1)  # Add an additional dimension to t\n",
    "#         inv_freq = 1.0 / (10000 ** (jnp.arange(0, channels, 2).astype(jnp.float32) / channels))\n",
    "#         pos_enc_a = jnp.sin(t * inv_freq)\n",
    "#         pos_enc_b = jnp.cos(t * inv_freq)\n",
    "#         pos_enc = jnp.concatenate([pos_enc_a, pos_enc_b], axis=-1)\n",
    "#         return pos_enc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
